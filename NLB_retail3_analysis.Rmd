---
title: "NLB_retail3_analysis"
author: "Rok Hribar"
date: "31 1 2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# TODO basic info
## 0. How to Comment?
Please comment on the line with your initials. For example:
"## RH: Comment here "

## 1. Data import

Import data here:

```{r}
mydata <- read.csv("raw_data.csv", header=TRUE, sep = ";", dec = ",")
mydata <- mydata[-c(1), ] # remove the first row as we do not need it
nrow(mydata)
head(mydata)
```

## 2. Data cleanup

Here we are cleaning our data:

```{r}
# Lets only keep the surveys of the people for which we have the demographic information for (used as our reference point). We will therefore only keep the people who came to the end of our survey. https://www.1ka.si/d/sl/pomoc/prirocniki/statusi-enot-ustreznost-veljavnost-manjkajoce-vrednosti
# -3 ... oseba zapustila vprašalnik, -2 ... oseba, ni v našem starostnem rangu

library(dplyr)
library(naniar)
library(tidyr)

mydata <- mydata %>% replace_with_na(replace = list(XSPOL1 = c(-2, -3))) # checking if they came to the end, initial cleanup

names(mydata)[1] <- "Age" # WE NEED TO RENAME

mydata <- mydata %>% replace_with_na(replace = list(Age = c(-1))) # checking if they answered the start

# We can afford to remove people that did not answer to our key survey questions
# Cleanup for RQ5
mydata <- mydata %>% replace_with_na(replace = list(Q18 = c(-1), Q24 = c(-1)))

# Cleanup for RQ6
mydata <- mydata %>% replace_with_na(replace = list(Q19a = c(-1, -5), Q19b = c(-1, -5), Q19c = c(-1, -5), Q19d = c(-1, -5), Q19e = c(-1, -5), Q18 = c(-1), Q24 = c(-1)))
mydata <- mydata %>% replace_with_na(replace = list(XDS2a4 = c(-1, -5)))

# Cleanup for RQ7
mydata <- mydata %>% replace_with_na(replace = list(Q22 = c(-1, -5)))

# Cleanup for pricing testing
mydata <- mydata %>% replace_with_na(replace = list(Q26 = c(-1), Q27 = c(-1), Q28 = c(-1), Q29 = c(-1)))
mydata <- drop_na(mydata)

# ADDITIONAL CLEANUP! of outliers for segmentation


mydata <- mydata[-80, ] # RUN ONLY ONCE
mydata <- mydata[-63, ] # RUN ONLY ONCE

# ADDITIONAL CLEANUP for segmentation

nrow(mydata)
head(mydata)
```

## 3. Undersampling based on secondary resources

Here we wanted to under sample our data, because we thought we had too little of young families. Turns out we have enough of them (5.7 %) which is pretty similar to the data we found from 2016 (young families of 20-29 age was 3,9 %). Therefore we have decided that we will not under sample based on this criteria.

https://www.nijz.si/sites/www.nijz.si/files/uploaded/publikacije/letopisi/2017/1_demografija_2017.pdf
https://www.nijz.si/sites/www.nijz.si/files/uploaded/publikacije/letopisi/2016/2.2_porodi_in_rojstva_2016.pdf

## 4. Hypothesis testing (refer to the document on hypothesis)

### RQ: 3. Why are young people choosing other financial technology solutions instead of traditional banking solutions?
### H: Young people use financial technology solutions because of its price.
### T: one way anova

Consultation Notes:
- repeated measures to see what stands out, 
- try to run factor analysis on Q14, skip the Q14f
- Assumption: a-d into one factor, e into another one
- We can group 4 together and mark it as convenience.
- testing the differences on Q3 on those that have a digital bank and those who do not have => t-test 
- logistic regression do it in MVA setting
- use factor analysis to see if they go together, a-b will go to one factor etc

- the reason why do they use it => explanatory variable
- binary logistic regression, based on if they use binary logistic regression based on employment factors

```{r}
# idk, we have here numerical scale

# hočem ugotoviti, kateri je najbolj vplival!:)
# avg za vsakega uporabnika
# potem primerjaš med odgovori

# 1. Cleanup

myDataReasons <- mydata %>% replace_with_na(replace = list(Q14a = c(-2, -3, -99), Q14b = c(-2, -3, -99), Q14c = c(-2, -3, -99), Q14d = c(-2, -3, -99), Q14e = c(-2, -3, -99)))

myDataReasons <- drop_na(myDataReasons)
nrow(myDataReasons) # 83 makes sense as data is cleaned

myDataReasons$Q14a <- as.numeric(as.character(myDataReasons$Q14a))
myDataReasons$Q14b <- as.numeric(as.character(myDataReasons$Q14b))
myDataReasons$Q14c <- as.numeric(as.character(myDataReasons$Q14c))
myDataReasons$Q14d <- as.numeric(as.character(myDataReasons$Q14d))
myDataReasons$Q14e <- as.numeric(as.character(myDataReasons$Q14e))

# myDataReasons$Q14a
myDataReasons <- data.frame(myDataReasons$Q14a, myDataReasons$Q14b, myDataReasons$Q14c, myDataReasons$Q14d, myDataReasons$Q14e)

myDataReasons <- tibble::rowid_to_column(myDataReasons, "ID")

head(myDataReasons)

```

```{r}
library(rstatix)
library(tidyverse)

names(myDataReasons)[2] <- "Spletni_nakupi"
names(myDataReasons)[3] <- "Brezplacni_dvigi"
names(myDataReasons)[4] <- "Enostavna_aplikacija"
names(myDataReasons)[5] <- "Brezplacna_aplikacija"
names(myDataReasons)[6] <- "Priporocilo_prijateljev"

myDataReasons_long <- myDataReasons %>%
                 gather(key = "Reason", value = "Importance", Spletni_nakupi, Brezplacni_dvigi, Enostavna_aplikacija, Brezplacna_aplikacija, Priporocilo_prijateljev) %>%
                 convert_as_factor(Reason)


tail(myDataReasons_long, 10)
```

```{r}
library(ggpubr)
ggboxplot(myDataReasons_long, 
          x = "Reason", 
          y = "Importance", 
          add = "jitter")

```

### Repeated measures ANOVA
```{r}
library(tidyverse)
library(ggpubr)
library(rstatix)

head(myDataReasons_long)

myDataReasons_long %>%
  group_by(Reason) %>%
  identify_outliers(Importance)

```
```{r}

myDataReasons_long <- myDataReasons_long %>%
  filter(!ID == 3, !ID == 11, !ID == 12, !ID == 18, !ID == 24, !ID == 40, !ID == 51, !ID == 53, !ID == 54, !ID == 56, !ID == 57, !ID == 61, !ID == 63, !ID == 3, !ID == 20, !ID == 41, !ID == 57, !ID == 61, !ID == 63, !ID == 3, !ID == 11, !ID == 12, !ID == 23, !ID == 26, !ID == 27, !ID == 36, !ID == 57, !ID == 61, !ID == 63, !ID == 74)

```


```{r}

myDataReasons_long %>%
  group_by(Reason) %>%
  shapiro_test(Importance)


ggqqplot(myDataReasons_long, "Importance", facet.by = "Reason")

```
```{r}
library(rstatix)
myDataReasons_long %>%
  group_by(Reason) %>%
  get_summary_stats(Importance, type = "common")

```

```{r}
ANOVA_results <- anova_test(dv = Importance, #Dependent variable
                            wid = ID, #Subject identifier
                            within = Reason, #Within-subject factor variable
                            data = myDataReasons_long)

get_anova_table(ANOVA_results)


```

```{r}
pwc <- myDataReasons_long %>%
  pairwise_t_test(Importance ~ Reason, 
                  paired = TRUE,
                  p.adjust.method = "bonferroni")

pwc
```

```{r}
library(rstatix)
pwc <- pwc %>% 
       add_y_position(fun = "median", step.increase = 0.35)

library(ggpubr)
ggboxplot(myDataReasons_long, x = "Reason", y = "Importance", add = "point", ylim=c(0, 18)) +
  stat_pvalue_manual(pwc, hide.ns = FALSE) +
  stat_summary(fun = mean, geom = "point", shape = 16, size = 4,
               aes(group = Reason), color = "darkred",
               position = position_dodge(width = 0.8)) +
  stat_summary(fun = mean, colour = "darkred", 
               position = position_dodge(width = 0.8),
               geom = "text", vjust = 0.5, hjust = -2,
               aes(label = round(..y.., digits = 2), group = Reason)) +
  labs(subtitle = get_test_label(ANOVA_results,  detailed = TRUE),
       caption = get_pwc_label(pwc))
```


### RQ: 1. What percentage of young people use digital banking?
### H: More than 60 % of young people use digital banking. 
### T: test of proportions

```{r}
# 1. Hypothesis's
# H0: p = 0.6
# H1: p =| 0.6

# 2. Data Selecting

mydata$Q16 <- as.numeric(as.character(mydata$Q16))

# 3. Getting the relevant data

counterNDigital <- 0

for(i in 1:length(mydata$Q16)) {
  if(mydata$Q16[i] == 1) {
    counterNDigital <- counterNDigital + 1
  }
}

counterNDigital
length(mydata$Q16)

# 4. Testing whether we need Binomial or prop test

pi0_H1 <- 0.58
pi0_H1 * length(mydata$Q16) > 5 && length(mydata$Q16)* (1 - pi0_H1) > 5

# We can use prop test as above condition holds true.

prop.test(x = counterNDigital, n = length(mydata$Q16), p = pi0_H1, correct = FALSE, alternative = "greater")

# VERDICT: We can overthrow null hypothesis and accept the H1 hypothesis, p-value = 0.04748

```

### RQ: 5. Are young people prepared to pay more for a sustainable banking package compared to a standard banking package?
### H: 60 % of young people would be prepared to pay more for sustainable products.
### T: test of proportions

```{r}

# 1. Hypothesis's
# H0: p = 0.6
# H1: p =| 0.6

# 2. Data Selecting
myDataPriceNormal <- as.numeric(as.character(mydata[ ,"Q18"]))
myDataPriceSustainable <- as.numeric(as.character(mydata[ ,"Q24"]))

length(myDataPriceSustainable) == length(myDataPriceNormal)

sampleSize <- length(myDataPriceNormal) # max. of people on clean data
# myDataPriceNormal

# 3. Getting the relevant data
counter <- 0

for(normalP in 1:length(myDataPriceNormal)) {
  if(myDataPriceNormal[normalP] < myDataPriceSustainable[normalP]) {
    counter <- counter + 1
  }
}

# counter
# sampleSize

# 4. Testing whether we need Binomial or prop test

pi0 <- 0.6
pi0 * sampleSize > 5 && sampleSize * (1 - pi0) > 5

# We can use prop test as above condition holds true.

prop.test(x = counter, n = sampleSize, p = pi0, correct = FALSE, alternative = "greater")

# VERDICT: We can overthrow null hypothesis and accept the H1 hypothesis, p-value = 0.02792.

                                     
```

### RQ: 6. How does the importance of sustainability differ among subgroups (students, employed, young families) in the group of young people?
### H: Students on average see sustainability as more important compared to employed people.
### T: independent t-tests

```{r}

# employed / unemployed Q: XDS2a4
# Q19 measures importance of sustainability for people

# 2. Data Selection

employmentData <- as.numeric(as.character(mydata[ ,"XDS2a4"]))

# Data cleaned!

# 3. Getting relevant data

# Let's calculate the average view on sustainability per person

viewsOnSustainabilityQ19a <- as.numeric(as.character(mydata[, "Q19a"])) 
viewsOnSustainabilityQ19b <- as.numeric(as.character(mydata[, "Q19b"]))
viewsOnSustainabilityQ19c <- as.numeric(as.character(mydata[, "Q19c"]))
viewsOnSustainabilityQ19d <- as.numeric(as.character(mydata[, "Q19d"]))
viewsOnSustainabilityQ19e <- as.numeric(as.character(mydata[, "Q19e"]))

viewsOnSustainability <- as.numeric(as.character(mydata[, "Q19e"])) # Dummy assign

# Here i just calculated the average of all 5 questions when we measured sustainability 
for(i in 1:length(viewsOnSustainabilityQ19a)) {
  viewsOnSustainability[i] <- (viewsOnSustainabilityQ19a[i] + viewsOnSustainabilityQ19b[i] + viewsOnSustainabilityQ19c[i] + viewsOnSustainabilityQ19d[i] + viewsOnSustainabilityQ19e[i]) / 5
}

# length(viewsOnSustainability)
# employmentData

viewsOnSustainabilityReference <- data.frame(mydata["Q19a"], mydata["Q19b"], mydata["Q19c"], mydata["Q19d"], mydata["Q19e"])

# viewsOnSustainabilityReference

viewsOnSustainabilityNew <- data.frame(employmentData, viewsOnSustainability)

# viewsOnSustainabilityNew

viewsOnSustainabilityNew$EmployedF <- factor(viewsOnSustainabilityNew$employmentData, levels = c(1, 2), labels = c("Yes", "No")) 

library(psych)
describeBy(viewsOnSustainabilityNew$viewsOnSustainability, viewsOnSustainabilityNew$EmployedF)
# viewsOnSustainabilityNew
```

```{r}

library(ggplot2)

Employed_no <- ggplot(viewsOnSustainabilityNew[viewsOnSustainabilityNew$EmployedF=="No", ], aes(x = viewsOnSustainability)) + theme_linedraw() + geom_histogram() + ggtitle("Not employed")

# Employed_no

Employed_yes <- ggplot(viewsOnSustainabilityNew[viewsOnSustainabilityNew$EmployedF=="Yes", ], aes(x = viewsOnSustainability)) + theme_linedraw() + geom_histogram() + ggtitle("Employed")

#Employed_yes

library(ggpubr)

ggarrange(Employed_no, Employed_yes, ncol = 2, nrow = 1)
```

```{r}

ggqqplot(viewsOnSustainabilityNew, "viewsOnSustainability", facet.by = "EmployedF")
# some variables should probably be removed
```

```{r}
t.test(viewsOnSustainabilityNew$viewsOnSustainability ~ viewsOnSustainabilityNew$EmployedF, paired = FALSE, var.equal = FALSE, alternative = "two.sided")

# VERDICT: We cannot reject  H0 (p = 0.3509), this makes sense because means are very close. TODO: How can we reject it?:)

```

### RQ: 7. What percentage of young people is interested in using sustainable financial products?
### H: More than 60% of young people would rather use a financial package that includes sustainable options compared to a financial package without sustainable options.
### T: test of proportions

```{r}

# 1. Hypothesis's
# H0: p = 0.6
# H1: p =| 0.6

# 2. Data Selection
myDataH7 <- as.numeric(as.character(mydata[ ,"Q22"]))

# myDataH7

myDataH7SampleSize <- length(myDataH7)

# 3. Getting the relevant data

counterH7 <- 0

for(i in 1:length(myDataH7)) { # run only once, unless you know what u are doing
  if(myDataH7[i] == 1) {
    counterH7 <- counterH7 + 1
  }
}

counterH7

# 4. Testing whether we need Binomial or prop test

pi0_h7 <- 0.6
pi0_h7 * myDataH7SampleSize > 5 && myDataH7SampleSize * (1 - pi0_h7) > 5

# We can use prop test as above condition holds true.

prop.test(x = counterH7, n = myDataH7SampleSize, p = pi0_h7, correct = FALSE, alternative = "greater")

# VERDICT: We can overthrow null hypothesis and accept the H1 hypothesis, p-value = 0.006822
```

## 5. Perception testing

### RQ: 6. How do young people perceive different banking solutions on the Slovenian market?
### H: Young people perceive NLB to have the best brand reputation.
### T: logistic regression model, rather use perception mapping

Consultations Notes:
- rather use perception mapping
- for each segments, repeated measures test for each of Q(parametric one, see if it systematically the best)


```{r}

# TAKE 1: Just take the cleaned averages from 1ka duhhhh

dostopnostEnka <- c(4.5, 5.8, 4.9, 4.1, 5.1, 5.3, 4.4)
cenaEnka <- c(4.6, 4.9, 4.6, 3.9, 3.8, 4.4, 3.8)
ugledEnka <- c(4.8, 5.2, 5.0, 4.5, 5.2, 5.5, 4.1)
odgovornostEnka <- c(4.5, 5.0, 4.6, 4.3, 4.5, 4.8, 4.5)


finalEnka <- data.frame(dostopnostEnka, cenaEnka, ugledEnka, odgovornostEnka)
rownames(finalEnka) <- c("NKBM", "NLB", "SKB", "DH", "N26", "Revolut", "mBills")
finalEnka
```

```{r}

# TAKE 2: Fully cleaned data
pc.percEnka <- princomp(finalEnka, cor = TRUE)
# pc.percEnka

# biplot(pc.percEnka) => done later

library(ggpubr)

# TAKE 2: Do our own analysis

# 1. Cleaning the data

# Q4 - dostopnost, 
# Q5 - cena, 
# Q6 - ugled, 
# Q7 - družbena in okoljska odgovornost

myDataPerceptionQ4 <- mydata %>% replace_with_na(replace = list(Q4a = c(-1, -5, -99), Q4b = c(-1, -5, -99), Q4c = c(-1, -5, -99), Q4d = c(-1, -5, -99), Q4e = c(-1, -5, -99), Q4f = c(-1, -5, -99), Q4g = c(-1, -5, -99)))

myDataPerceptionQ5 <- mydata %>% replace_with_na(replace = list(Q5a = c(-1, -5, -99), Q5b = c(-1, -5, -99), Q5c = c(-1, -5, -99), Q5d = c(-1, -5, -99), Q5e = c(-1, -5, -99), Q5f = c(-1, -5, -99), Q5g = c(-1, -5, -99)))

myDataPerceptionQ6 <- mydata %>% replace_with_na(replace = list(Q6a = c(-1, -5, -99), Q6b = c(-1, -5, -99), Q6c = c(-1, -5, -99), Q6d = c(-1, -5, -99), Q6e = c(-1, -5, -99), Q6f = c(-1, -5, -99), Q6g = c(-1, -5, -99)))

myDataPerceptionQ7 <- mydata %>% replace_with_na(replace = list(Q7a = c(-1, -5, -99), Q7b = c(-1, -5, -99), Q7c = c(-1, -5, -99), Q7d = c(-1, -5, -99), Q7e = c(-1, -5, -99), Q7f = c(-1, -5, -99), Q7g = c(-1, -5, -99)))


myDataPerceptionQ4dropped <- drop_na(myDataPerceptionQ4)
nrow(myDataPerceptionQ4dropped) # 39 here

myDataPerceptionQ5dropped <- drop_na(myDataPerceptionQ5)
nrow(myDataPerceptionQ5dropped) # 22 here

myDataPerceptionQ6dropped <- drop_na(myDataPerceptionQ6)
nrow(myDataPerceptionQ6dropped) # 56 here

myDataPerceptionQ7dropped <- drop_na(myDataPerceptionQ7)
nrow(myDataPerceptionQ7dropped) # 29 here

NKBM_dostopnost <- mean(as.numeric(as.character(myDataPerceptionQ4dropped[, "Q4a"])))
NLB_dostopnost <- mean(as.numeric(as.character(myDataPerceptionQ4dropped[, "Q4b"])))
SKB_dostopnost <- mean(as.numeric(as.character(myDataPerceptionQ4dropped[, "Q4c"])))
DH_dostopnost <- mean(as.numeric(as.character(myDataPerceptionQ4dropped[, "Q4d"])))
N26_dostopnost <- mean(as.numeric(as.character(myDataPerceptionQ4dropped[, "Q4e"])))
Rev_dostopnost <- mean(as.numeric(as.character(myDataPerceptionQ4dropped[, "Q4f"])))
Bills_dostopnost <- mean(as.numeric(as.character(myDataPerceptionQ4dropped[, "Q4g"])))

NKBM_ugled <- mean(as.numeric(as.character(myDataPerceptionQ5dropped[, "Q5a"])))
NLB_ugled <- mean(as.numeric(as.character(myDataPerceptionQ5dropped[, "Q5b"])))
SKB_ugled <- mean(as.numeric(as.character(myDataPerceptionQ5dropped[, "Q5c"])))
DH_ugled <- mean(as.numeric(as.character(myDataPerceptionQ5dropped[, "Q5d"])))
N26_ugled <- mean(as.numeric(as.character(myDataPerceptionQ5dropped[, "Q5e"])))
Rev_ugled <- mean(as.numeric(as.character(myDataPerceptionQ5dropped[, "Q5f"])))
Bills_ugled <- mean(as.numeric(as.character(myDataPerceptionQ5dropped[, "Q5g"])))

NKBM_cena <- mean(as.numeric(as.character(myDataPerceptionQ6dropped[, "Q6a"])))
NLB_cena <- mean(as.numeric(as.character(myDataPerceptionQ6dropped[, "Q6b"])))
SKB_cena <- mean(as.numeric(as.character(myDataPerceptionQ6dropped[, "Q6c"])))
DH_cena <- mean(as.numeric(as.character(myDataPerceptionQ6dropped[, "Q6d"])))
N26_cena <- mean(as.numeric(as.character(myDataPerceptionQ6dropped[, "Q6e"])))
Rev_cena <- mean(as.numeric(as.character(myDataPerceptionQ6dropped[, "Q6f"])))
Bills_cena <- mean(as.numeric(as.character(myDataPerceptionQ6dropped[, "Q6g"])))

NKBM_odgovornost <- mean(as.numeric(as.character(myDataPerceptionQ7dropped[, "Q7a"])))
NLB_odgovornost <- mean(as.numeric(as.character(myDataPerceptionQ7dropped[, "Q7b"])))
SKB_odgovornost <- mean(as.numeric(as.character(myDataPerceptionQ7dropped[, "Q7c"])))
DH_odgovornost <- mean(as.numeric(as.character(myDataPerceptionQ7dropped[, "Q7d"])))
N26_odgovornost <- mean(as.numeric(as.character(myDataPerceptionQ7dropped[, "Q7e"])))
Rev_odgovornost <- mean(as.numeric(as.character(myDataPerceptionQ7dropped[, "Q7f"])))
Bills_odgovornost <- mean(as.numeric(as.character(myDataPerceptionQ7dropped[, "Q7g"])))


dostopnost <- c(NKBM_dostopnost, NLB_dostopnost, SKB_dostopnost, DH_dostopnost, N26_dostopnost, Rev_dostopnost, Bills_dostopnost)
cena <- c(NKBM_cena, NLB_cena, SKB_cena, DH_cena, N26_cena, Rev_cena, Bills_cena)
ugled <- c(NKBM_ugled, NLB_ugled, SKB_ugled, DH_ugled, N26_ugled, Rev_ugled, Bills_ugled)
odgovornost <- c(NKBM_odgovornost, NLB_odgovornost, SKB_odgovornost, DH_odgovornost, N26_odgovornost, Rev_odgovornost, Bills_odgovornost)


final <- data.frame(dostopnost, cena, ugled, odgovornost)
rownames(final) <- c("NKBM", "NLB", "SKB", "DH", "N26", "Revolut", "mBills")
final

# Na instead of -99, the average correctly
# mean()

```

```{r}
# TAKE 3: Calculate the AVG with NA

library(ggpubr)

myDataPerceptionQ4NA <- mydata %>% replace_with_na(replace = list(Q4a = c(-1, -5, -99), Q4b = c(-1, -5, -99), Q4c = c(-1, -5, -99), Q4d = c(-1, -5, -99), Q4e = c(-1, -5, -99), Q4f = c(-1, -5, -99), Q4g = c(-1, -5, -99)))

myDataPerceptionQ5NA <- mydata %>% replace_with_na(replace = list(Q5a = c(-1, -5, -99), Q5b = c(-1, -5, -99), Q5c = c(-1, -5, -99), Q5d = c(-1, -5, -99), Q5e = c(-1, -5, -99), Q5f = c(-1, -5, -99), Q5g = c(-1, -5, -99)))

myDataPerceptionQ6NA <- mydata %>% replace_with_na(replace = list(Q6a = c(-1, -5, -99), Q6b = c(-1, -5, -99), Q6c = c(-1, -5, -99), Q6d = c(-1, -5, -99), Q6e = c(-1, -5, -99), Q6f = c(-1, -5, -99), Q6g = c(-1, -5, -99)))

myDataPerceptionQ7NA <- mydata %>% replace_with_na(replace = list(Q7a = c(-1, -5, -99), Q7b = c(-1, -5, -99), Q7c = c(-1, -5, -99), Q7d = c(-1, -5, -99), Q7e = c(-1, -5, -99), Q7f = c(-1, -5, -99), Q7g = c(-1, -5, -99)))

NKBM_dostopnostNA <- mean(as.numeric(as.character(myDataPerceptionQ4NA[, "Q4a"])), na.rm = TRUE)
NLB_dostopnostNA <- mean(as.numeric(as.character(myDataPerceptionQ4NA[, "Q4b"])), na.rm = TRUE)
SKB_dostopnostNA <- mean(as.numeric(as.character(myDataPerceptionQ4NA[, "Q4c"])), na.rm = TRUE)
DH_dostopnostNA <- mean(as.numeric(as.character(myDataPerceptionQ4NA[, "Q4d"])), na.rm = TRUE)
N26_dostopnostNA <- mean(as.numeric(as.character(myDataPerceptionQ4NA[, "Q4e"])), na.rm = TRUE)
Rev_dostopnostNA <- mean(as.numeric(as.character(myDataPerceptionQ4NA[, "Q4f"])), na.rm = TRUE)
Bills_dostopnostNA <- mean(as.numeric(as.character(myDataPerceptionQ4NA[, "Q4g"])), na.rm = TRUE)

NKBM_ugledNA <- mean(as.numeric(as.character(myDataPerceptionQ5NA[, "Q5a"])), na.rm = TRUE)
NLB_ugledNA <- mean(as.numeric(as.character(myDataPerceptionQ5NA[, "Q5b"])), na.rm = TRUE)
SKB_ugledNA <- mean(as.numeric(as.character(myDataPerceptionQ5NA[, "Q5c"])), na.rm = TRUE)
DH_ugledNA <- mean(as.numeric(as.character(myDataPerceptionQ5NA[, "Q5d"])), na.rm = TRUE)
N26_ugledNA <- mean(as.numeric(as.character(myDataPerceptionQ5NA[, "Q5e"])), na.rm = TRUE)
Rev_ugledNA <- mean(as.numeric(as.character(myDataPerceptionQ5NA[, "Q5f"])), na.rm = TRUE)
Bills_ugledNA <- mean(as.numeric(as.character(myDataPerceptionQ5NA[, "Q5g"])), na.rm = TRUE)

NKBM_cenaNA <- mean(as.numeric(as.character(myDataPerceptionQ6NA[, "Q6a"])), na.rm = TRUE)
NLB_cenaNA <- mean(as.numeric(as.character(myDataPerceptionQ6NA[, "Q6b"])), na.rm = TRUE)
SKB_cenaNA <- mean(as.numeric(as.character(myDataPerceptionQ6NA[, "Q6c"])), na.rm = TRUE)
DH_cenaNA <- mean(as.numeric(as.character(myDataPerceptionQ6NA[, "Q6d"])), na.rm = TRUE)
N26_cenaNA <- mean(as.numeric(as.character(myDataPerceptionQ6NA[, "Q6e"])), na.rm = TRUE)
Rev_cenaNA <- mean(as.numeric(as.character(myDataPerceptionQ6NA[, "Q6f"])), na.rm = TRUE)
Bills_cenaNA <- mean(as.numeric(as.character(myDataPerceptionQ6NA[, "Q6g"])), na.rm = TRUE)

NKBM_odgovornostNA <- mean(as.numeric(as.character(myDataPerceptionQ7NA[, "Q7a"])), na.rm = TRUE)
NLB_odgovornostNA <- mean(as.numeric(as.character(myDataPerceptionQ7NA[, "Q7b"])), na.rm = TRUE)
SKB_odgovornostNA <- mean(as.numeric(as.character(myDataPerceptionQ7NA[, "Q7c"])), na.rm = TRUE)
DH_odgovornostNA <- mean(as.numeric(as.character(myDataPerceptionQ7NA[, "Q7d"])), na.rm = TRUE)
N26_odgovornostNA <- mean(as.numeric(as.character(myDataPerceptionQ7NA[, "Q7e"])), na.rm = TRUE)
Rev_odgovornostNA <- mean(as.numeric(as.character(myDataPerceptionQ7NA[, "Q7f"])), na.rm = TRUE)
Bills_odgovornostNA <- mean(as.numeric(as.character(myDataPerceptionQ7NA[, "Q7g"])), na.rm = TRUE)

dostopnostNA <- c(NKBM_dostopnostNA, NLB_dostopnostNA, SKB_dostopnostNA, DH_dostopnostNA, N26_dostopnostNA, Rev_dostopnostNA, Bills_dostopnostNA)
cenaNA <- c(NKBM_cenaNA, NLB_cenaNA, SKB_cenaNA, DH_cenaNA, N26_cenaNA, Rev_cenaNA, Bills_cenaNA)
ugledNA <- c(NKBM_ugledNA, NLB_ugledNA, SKB_ugledNA, DH_ugledNA, N26_ugledNA, Rev_ugledNA, Bills_ugledNA)
odgovornostNA <- c(NKBM_odgovornostNA, NLB_odgovornostNA, SKB_odgovornostNA, DH_odgovornostNA, N26_odgovornostNA, Rev_odgovornostNA, Bills_odgovornostNA)

Convenience <- dostopnostNA
Price <- cenaNA
Brand_Reputation <- ugledNA
Responsibility <- odgovornostNA

finalNA <- data.frame(Convenience, Price, Brand_Reputation, Responsibility)
rownames(finalNA) <- c("NKBM", "NLB", "SKB", "DH", "N26", "Revolut", "mBills")
ugledNA
finalNA


pc.percNA <- princomp(finalNA, cor = TRUE)
```


```{r}

pc.perc <- princomp(final, cor = TRUE)

ggarrange(biplot(pc.perc), biplot(pc.percEnka), biplot(pc.percNA), ncol = 3, nrow = 1)

```

## 6. Pricing testing

Lets look at our Van Westendorp pricing analysis. In our survey VW analysis was measured with the following Qs: Q26	Q27	Q28	Q29.

```{r}
library(ggplot2)

myDataVW <- mydata

# DATA CLEANUP: We need to clean the data for these for Qs. We need to filter the people who did not answer this Qs.

myDataVW <- myDataVW[-8, ]
myDataVW <- myDataVW[-213, ]
myDataVW <- myDataVW[-196, ]
myDataVW <- myDataVW[-175, ]
myDataVW <- myDataVW[-168, ]
myDataVW <- myDataVW[-103, ]
myDataVW <- myDataVW[-78, ]
myDataVW <- myDataVW[-33, ]


# Van Westerdorp analysis TAKE 2
myDataVW$Q26 <- as.numeric(as.character(myDataVW$Q26))
myDataVW$Q27 <- as.numeric(as.character(myDataVW$Q27))
myDataVW$Q28 <- as.numeric(as.character(myDataVW$Q28))
myDataVW$Q29 <- as.numeric(as.character(myDataVW$Q29))


library(pricesensitivitymeter)

psm_out <- psm_analysis(toocheap = myDataVW$Q26, 
             cheap = myDataVW$Q27, 
             expensive = myDataVW$Q28,
             tooexpensive = myDataVW$Q29,
             validate = TRUE)

str(psm_out)
head(psm_out$data_input)
head(psm_out$data_vanwestendorp)

psmplot <-  ggplot(data = psm_out$data_vanwestendorp, aes(x = price)) +
  annotate(geom = "rect", # shaded background area for range of acceptable prices
           xmin = psm_out$pricerange_lower,
           xmax = psm_out$pricerange_upper,
           ymin = 0, ymax = Inf,
           fill="grey50", alpha = 0.3) +
  geom_line(aes(y = ecdf_toocheap, # line: too cheap
                colour = "too cheap",
                linetype = "too cheap"),
            size= .5) +
  geom_line(aes(y = ecdf_tooexpensive, # line: too expensive
                colour = "too expensive",
                linetype = "too expensive"),
            size = .5) + 
  geom_line(aes(y = ecdf_not_cheap, # line: not cheap
                colour = "not cheap",
                linetype = "not cheap"),
            size = .5) +
  geom_line(aes(y = ecdf_not_expensive, # line: not expensive
                colour = "not expensive",
                linetype = "not expensive"),
            size = .5) + 
  annotate(geom = "point", # Indifference Price Point (intersection of "cheap" and "expensive")
           x = psm_out$idp, 
           y = psm_out$data_vanwestendorp$ecdf_not_cheap[psm_out$data_vanwestendorp$price == psm_out$idp],
           size = 3,
           shape = 2,
           colour = "#D55E00",
           fill = "#D55E00",
           alpha = .8) + 
  annotate(geom = "point", # Optimal Price Point (intersection of "too cheap" and "too expensive")
           x = psm_out$opp, 
           y = psm_out$data_vanwestendorp$ecdf_toocheap[psm_out$data_vanwestendorp$price == psm_out$opp],
           size = 3,
           shape = 2,
           colour = "#D55E00",
           fill = "#D55E00",
           alpha = 0.8)


# Update plot with labels and colors
psmplot +
  labs(x = "Price",
       y = "Proportion of Respondents",
       title = "Price Sensitivity Plot for Product X",
       caption = paste("sample size:",
                       psm_out$total_sample - psm_out$invalid_cases,
                       "respondents"))  + 
  scale_colour_manual(name = "Legend",
                      values = c("too cheap" = "#040453",
                                 "not cheap" = "#040453",
                                 "not expensive" = "#00b0f0",
                                 "too expensive" = "#00b0f0")) + 
  scale_linetype_manual(name="Legend",
                        values = c("too cheap" = "dotted",
                                   "not cheap" = "solid",
                                   "not expensive" = "solid",
                                   "too expensive" = "dotted")) + 
  annotate(geom = "text", # Label of Indifference Price Point
           x = psm_out$idp + 5, 
           y = psm_out$data_vanwestendorp$ecdf_not_cheap[psm_out$data_vanwestendorp$price == psm_out$idp] + .05,
           label = paste("IDP: ", psm_out$idp)) + 
  annotate(geom = "text", # Label of Optimal Price Point
           x = psm_out$opp,
           y = psm_out$data_vanwestendorp$ecdf_toocheap[psm_out$data_vanwestendorp$price == psm_out$opp] + .05,
           label = paste("OPP: ", psm_out$opp)) +
  theme_minimal()

myDataPriceFourQ <- data.frame(myDataVW$Q26, myDataVW$Q27, myDataVW$Q28, myDataVW$Q29)



```

## 7. Social proof testing

Here we are cleaning our data:

```{r}
# TODO: Wont Do.

# use correlation between them and see if they are very similar you group them

# Box plots for each product specific product
# Just show them graphically
# x products, y values from 1-7

# t-test for each product, to see if 5.1 is statistically signifanct than 4
# t-test for one arithmetic mean

# test of propotions, more than half of people would be interested in this product, cutoff 4
```

## 8. Additional important key parts (hypothesis)

Here we are cleaning our data:

```{r}
# TODO: Wont Do.
# They ARE prepared to pay more!?
# Why DOES 1/3 not use the packages!?
# Do people see n26 as more secure for online shopping!?
# Include case studies from abroad

```

## 9. Segmentation

# I want to segment our customers

#### We need to segment the customers in different segments. 
#### Our variables will be:

#### Max 5 variables: (iteracija 10)
#### How sustainable is your lifestyle? (stands for Attitude) => avg. 1-7
#### How much do you consider sustainability when making purchases? 
#### How much are you prepared to use sustainable packages? => 1-7

#### How much are you willing to pay for a normal package: Q18
#### How much are you willing to pay for a sustainable package: Q24 


#### How much do you use financial products? (stands for Usage) => avg. 1- => Didnt use

```{r}

segmentationData <- mydata

# How much do you use financial products? (stands for Usage) => avg. 1-7 => maybe added maybe not
# How sustainable is your lifestyle? (stands for Attitude) => avg. 1-7
viewsOnSustainability
typeof(viewsOnSustainability)
is.numeric(viewsOnSustainability)
# segmentationData

segmentationData["LS_avg"] <- viewsOnSustainability
# segmentationData["LS_avg"]

# Q21: How much do you consider sustainability when making purchases? 
nrow(segmentationData[c("Q21a")])
# How much are you prepared to use sustainable packages? => 1-7
segmentationData["Q23b"] <- as.numeric(as.character(segmentationData[ ,"Q23b"]))
# segmentationData["Q23b"]

# How much are you willing to pay for a normal package: Q18
segmentationData["Q18"] <- as.numeric(as.character(segmentationData[ ,"Q18"]))
# segmentationData["Q18"]

# How much are you willing to pay for a sustainable package: Q24 
segmentationData["Q24"] <- as.numeric(as.character(segmentationData[ ,"Q24"]))
# segmentationData["Q24"]


summary(segmentationData[c("LS_avg", "Q21a", "Q23b", "Q18", "Q24")])

segmentationData_std <- as.data.frame(scale(segmentationData[c("LS_avg", "Q21a", "Q23b", "Q18", "Q24")]))

# segmentationData_std

# Finding outliers
segmentationData_std$Dissimilarity = sqrt(segmentationData_std$LS_avg^2 + segmentationData_std$Q21a^2 + segmentationData_std$Q23b^2 + segmentationData_std$Q18^2 + segmentationData_std$Q24^2)

head(segmentationData_std[order(-segmentationData_std$Dissimilarity), ], 5)

# print(segmentationData[82, ]) # Checking outliers

# nrow(segmentationData)

```

```{r}

segmentationData_std_NoDissimilarity <- segmentationData_std # this step is REQUIRED, REMOVE DISSIMILARITY BEFORE DOING FURTHER ANALYSIS
segmentationData_std_NoDissimilarity$Dissimilarity <- NULL

round(cor(segmentationData_std_NoDissimilarity), 2)

```

```{r}
library(factoextra)

Distances <- get_dist(segmentationData_std_NoDissimilarity, method = "euclidian")

fviz_dist(Distances)

get_clust_tendency(segmentationData_std_NoDissimilarity, n = nrow(segmentationData_std_NoDissimilarity) - 1, graph = FALSE)

```

```{r}
library(dplyr)

(WARD <- segmentationData_std_NoDissimilarity %>% get_dist(method = "euclidian") %>% hclust(method = "ward.D2"))

fviz_dend(WARD)

fviz_dend(WARD, k = 3, palette = "Set1", rect = TRUE) # 15 proposed 3 as the best number of clusters

fviz_dend(WARD, k = 5, palette = "Set1", rect = TRUE)

fviz_dend(WARD, k = 4, palette = "Set1", rect = TRUE)

```

```{r}
set.seed(1)

library(NbClust)

Index <- NbClust(segmentationData_std_NoDissimilarity, distance = "euclidean", method = "ward.D2", index = "all", min.nc = 2, max.nc = 6)


segmentationData$Clustering_Ward <- cutree(WARD, k = 3)

head(segmentationData)

Leaders_initial <- aggregate(segmentationData_std_NoDissimilarity, by = list(segmentationData$Clustering_Ward), FUN = mean)

Leaders_initial

```

```{r}

(kmeans_clu <- hkmeans(segmentationData_std_NoDissimilarity, k = 3, hc.metric = "euclidean", hc.method = "ward.D2"))

```

```{r}
fviz_cluster(kmeans_clu, palette = "Set1", repel = FALSE, ggtheme = theme_linedraw())

```

```{r}

R <- cor(segmentationData_std_NoDissimilarity)

round(R, 3)

```

```{r}

library(psych)

cortest.bartlett(R, n = nrow(segmentationData_std_NoDissimilarity))

KMO(R)

```

```{r}

library(FactoMineR)
components <- PCA(segmentationData_std_NoDissimilarity, scale.unit = FALSE, graph = FALSE)

get_eigenvalue(components)

```

```{r}

fviz_eig(components, choice = "eigenvalue", main = "Screeplot", ylab = "Eigenvalues", xlab = "Principal Component", addlabels = TRUE)

```

```{r}

fa.parallel(segmentationData_std_NoDissimilarity, sim = FALSE, fa = "pc")

```

```{r}
(components <- PCA(segmentationData_std_NoDissimilarity, scale.unit = FALSE, graph = FALSE, ncp = 2))

print(components$var$cor)

fviz_pca_var(components, repel = TRUE)
```

```{r}

fviz_pca_biplot(components)

```

```{r}

segmentationData$Clustering_kmeans <- kmeans_clu$cluster

# Check if we need optimization
head(segmentationData) # => uneven

table(segmentationData$Clustering_Ward)
table(segmentationData$Clustering_kmeans)

table(segmentationData$Clustering_Ward, segmentationData$Clustering_kmeans)

```

```{r}

Leaders_final <- kmeans_clu$centers
Leaders_final

```

```{r}

# DESCRIBING CLUSTERS

Figure <- as.data.frame(Leaders_final)

Figure$ID <- 1:nrow(Figure)

library(tidyr)

Figure <- pivot_longer(Figure, cols = c("LS_avg", "Q21a", "Q23b", "Q18", "Q24"))

Figure$Group <- factor(Figure$ID, levels = c(1, 2, 3), labels = c("1", "2", "3"))

Figure$NameF <- factor(Figure$name,
                       levels = c("LS_avg", "Q21a", "Q23b", "Q18", "Q24"),
                       labels = c("LifeSust.", "Sust. Purchases", "UseOfSustPckg", "NormalPckgPrice", "SustPckgPrice"))

library(ggplot2)

ggplot(Figure, aes(x = NameF, y = value)) +
  geom_hline(yintercept = 0) +
  theme_linedraw() +
  geom_point(aes(shape = Group, col = Group), size = 3) +
  geom_line(aes(group = ID), size = 1) +
  ylab("Averages") +
  xlab("Cluster variables") +
  ylim(-2.2, 2.2)

# aware and willing, not aware but willing, not aware
```

```{r}

fit <- aov(cbind(LS_avg, Q21a, Q23b, Q18, Q24) ~ as.factor(Clustering_kmeans), data = segmentationData ) 
# TODO: Ask: WHICH DATA DO WE USE HERE? STD OR INITIAL????

summary(fit) # all p values are good

```
### VALIDATION

```{r}

# Further ( + descriptive) analysis needs to be perfomed

# PRIMARY FACTORS:
# GenderF
segmentationData$GenderF <- factor(segmentationData$XSPOL, 
                            levels = c(1, 2), 
                            labels = c("M", "F"))

# AgeF
segmentationData$AgeF <- factor(segmentationData$Age, 
                            levels = c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11), 
                            labels = c("18", "19", "20", "21", "22", "23", "24", "25", "26", "27"))

# NeobankUseF
segmentationData$NeobankUseF <- factor(segmentationData$Q16, 
                            levels = c(1, 2), 
                            labels = c("Yes", "No"))

# FrequencyOfDigitalPurchasesF
segmentationData$FrequencyOfDigitalPurchasesF <- factor(segmentationData$Q11a, 
                            levels = c(1, 2, 3, 4, 5), 
                            labels = c("Not at all", " Once per month", "Several times per month", "Several times per week", "Every day"))

# ADDITIONAL:
#DigitalBankF
segmentationData$DigitalBankF <- factor(segmentationData$Q16, 
                            levels = c(1, 2), 
                            labels = c("Yes", "No"))
# BankTypePreferenceF
segmentationData$BankTypePreferenceF <- factor(segmentationData$Q17, 
                            levels = c(1, 2), 
                            labels = c("Digital", "Traditional"))

# ImpactOnEnvironmentF
segmentationData$ImpactOnEnvironmentF <- factor(segmentationData$Q22, 
                            levels = c(1, 2), 
                            labels = c("Yes", "No"))
# EmploymentF
segmentationData$EmploymentF <- factor(segmentationData$XDS2a4, 
                            levels = c(1, 2), 
                            labels = c("Student", "Employed"))

# LivingParentsF
segmentationData$LivingParentsF <- factor(segmentationData$Q31, 
                            levels = c(1, 2), 
                            labels = c("Yes", "No"))

# KidsF
segmentationData$KidsF <- factor(segmentationData$XSPOL1, 
                            levels = c(1, 2), 
                            labels = c("Yes", "No"))

# SalaryF
segmentationData$SalaryF <- factor(segmentationData$Q30, 
                            levels = c(1, 2, 3, 4, 5, 6, 7), 
                            labels = c("Brez dohodka", "0-499", "500-999", "1000-1499", "1500-1999", "2000-2499", "2500+"))

# EducationF
segmentationData$EducationF <- factor(segmentationData$XIZ1a2, 
                            levels = c(1, 2, 3, 4, 5, 6, 7), 
                            labels = c("Srednja sola", "Visja sola", "Visoka sola", "Uni. sola", "Magister", "Doktorat", "Other"))




segmentationData$AgeF <- as.numeric(as.character(segmentationData$AgeF))

aggregate(segmentationData$AgeF,
          by = list(segmentationData$Clustering_kmeans),
          FUN = 'mean')


Fit <- aov(AgeF ~ as.factor(Clustering_kmeans), data= segmentationData)

summary(Fit)
```
#### H0: meanG1 = meanG2 = meanG3
#### H1: At least one mean is different.
#### We cannot reject H0 because p-value is too high. The means of the three groups are too similar to be statistically significant.
#### Calculating the effect size: r**2=22.5/(22.5 + 1317.5) = 0.017
#### Effect size is small, the differences are small. 

```{r}

segmentationData$Q11a <- as.numeric(as.character(segmentationData$Q11a))

aggregate(segmentationData$Q11a,
          by = list(segmentationData$Clustering_kmeans),
          FUN = 'mean')

Fit1 <- aov(Q11a ~ as.factor(Clustering_kmeans), data= segmentationData)

summary(Fit1)
```
#### H0: meanG1 = meanG2 = meanG3
#### H1: At least one mean is different.
#### We cannot reject H0 because p-value is too high. The means of the three groups are too similar to be statistically significant.

```{r}
chisq_resultsGen <- chisq.test(segmentationData$GenderF, as.factor(segmentationData$Clustering_kmeans))
chisq_resultsGen

addmargins(chisq_resultsGen$observed)

round(chisq_resultsGen$expected, 2)
```
#### This is acceptable because all values are above 5.
```{r}
round(chisq_resultsGen$res, 2)
```
#### No validation because none of these are higher than 1.64 in absolute terms - values are too low to be statistically significant.

```{r}
chisq_resultsImp <- chisq.test(segmentationData$ImpactOnEnvironmentF, as.factor(segmentationData$Clustering_kmeans))
chisq_resultsImp

addmargins(chisq_resultsImp$observed)
```

```{r}
round(chisq_resultsImp$expected, 2)

round(chisq_resultsImp$res, 2)
```
#### This is acceptable because all of these values are above 5 (expected values).

#### In Group1, there is less people than expected that answered NO. In Group2, there is more people than expected that answered NO. In Group3, there is no significant difference.

#### With the variable ImpactonEnvironmentF, we validated the results because the results were not what we expected.In Group 2, there are less people than expected who want to know about their impact on the environment and more people than expected who do not want to know about their impact.

```{r}
# Lets do Cramer test to see the effect size and interpret

library(DescTools)

CramerV(segmentationData$ImpactOnEnvironmentF, as.factor(segmentationData$Clustering_kmeans))

# can see that this is mid effect <= check theory and fix this verdict, 0.377

# Denis then did fisher test here

fisher.test(segmentationData$ImpactOnEnvironmentF, as.factor(segmentationData$Clustering_kmeans))
```

```{r}
chisq_resultsDig<- chisq.test(segmentationData$DigitalBankF, as.factor(segmentationData$Clustering_kmeans))
chisq_resultsDig

addmargins(chisq_resultsDig$observed)

round(chisq_resultsDig$expected, 2)

round(chisq_resultsDig$res, 2)
```
#### Values are too low to be statistically significant.

```{r}
chisq_resultsEmp <- chisq.test(segmentationData$EmploymentF, as.factor(segmentationData$Clustering_kmeans))
chisq_resultsEmp

addmargins(chisq_resultsEmp$observed)

round(chisq_resultsEmp$expected, 2)

round(chisq_resultsEmp$res, 2)
```
#### In Group 2, more people than expected are employed.In Group 1 and Group 3, the differences are too small to be statistically significant. 

```{r}
chisq_resultsLiv <- chisq.test(segmentationData$LivingParentsF, as.factor(segmentationData$Clustering_kmeans))
chisq_resultsLiv

addmargins(chisq_resultsLiv$observed)

round(chisq_resultsLiv$expected, 2)

round(chisq_resultsLiv$res, 2)
```
#### Values are too low to be statistically significant.



## We performed hierarchical clustering of 217 units based on 5 cluster variables (Squared Euclidean Distance, Ward Algorithm). The results were further optimized with k-means clustering.

#### Description of Cluster 1:

#### Name: GREEN PENNY-PINCHERS

#### Cluster 1 represents the largest group of customers (estimated at 41 %) who believe that sustainability is important. They lead a sustainable lifestyle on a daily basis, consider sustainability when making purchases and would be interested in using a sustainable financial solution integrated into their digital bank. In this group, there are fewer people than expected that said that they would not like to know their impact on the environment based on their spending habits, which further confirms their awareness about sustainability. These customers are not prepared to pay either for a normal banking package or for a sustainable banking package.In this group, there are 37 % of men and 63 % of women, 64 % use a digital bank and 36 % do not. The first group consists of 79 % students and 21 % of employed young individuals which could explain the fact that customers in this group are not prepared to pay for banking packages since students mostly have less money to spend than employed individuals.  

#### Description of Cluster 2:

#### Name: ANTI-GREENIES

#### Cluster 2 represents the smallest group of customers (estimated at 28 %) who are not aware of the importance of sustainability. They do not lead a sustainable lifestyle on a daily basis, do not consider sustainability when making purchases and would not be interested in using a sustainable financial solution integrated into their digital bank. More than expected, this group would not be interested in knowing about the impact their spending has on the environment, which further confirms them being unaware of the importance of sustainability. These customers are not prepared to pay for both a normal banking package as well as a sustainable banking package. In this group, there are 45 % of men and 55 % of women, 72 % use a digital bank and 28 % do not. The second group consists of 58 % of students and 42 % of employed young people. Considering the variable of employment, there are more employed young individuals in this group than expected, which can be explained by them being unaware of the importance of sustainability which they consider as a cause on which they are not prepared to spend their money.

#### Description of Cluster 3:

#### Name: GENEROUS ENVIRONMENTALISTS

#### Cluster 3 represents the second largest group of customers (estimated at 31 %) who believe that sustainability is important. They lead a sustainable lifestyle on a daily basis, consider sustainability when making purchases and would be interested in using a sustainable financial solution integrated into their digital bank. These customers are prepared to pay for both their normal banking package as well as a sustainable banking package.In this group, there are 39 % of men and 61 % of women, 55 % use a digital bank and 45 % do not. The third group consists of 78 % students and 22 % of employed young individuals, which is interesting because the group constitution based on employment is very similar to the first group, so these customers clearly consider sustainability as a cause that is worth spending money on since they are prepared to pay for a sustainable banking package.

